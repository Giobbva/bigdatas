# ðŸ§  Benchmarking Token Contamination in AI Agents
**Datathon EA UPY 2025 â€” July Edition**  
**Polytechnic University of YucatÃ¡n**

---

## ðŸ“˜ Overview
This project benchmarks **token contamination** across multiple AI agents (GPT-4o, Claude 3 Sonnet, Gemini, and DeepSeek).  
It provides a reproducible framework to **generate, evaluate, and analyze AI outputs** under standardized prompt conditions, using **LangChain**, **MongoDB**, and **OpenRouter / vendor APIs**.

> *Token contamination* refers to residual biases, repetition, or pattern artifacts that appear when AI outputs are reused in training data â€” potentially degrading future model performance or ethical neutrality.

---

## âš™ï¸ Architecture Overview
Automated evaluations run through a modular **pipeline**:

1. **Prompt ingestion** from structured datasets (`EA_Benchmark_Prompts_200.csv`).  
2. **Agent orchestration** via LangChain wrappers and native SDKs (OpenRouter â†”ï¸Ž GPT-4o/Claude, Google Generative AI â†”ï¸Ž Gemini, DeepSeek SDK).  
3. **Response storage** in **MongoDB**, separated by model collections.  
4. **Metric computation** (redundancy, entropy, semantic similarity).  
5. **Benchmark reports** and visual analyses in a Jupyter Notebook.

---

## ðŸ§© Core Components (by file)

### `src/invoke.py` â€” Model bootstrap
- Initializes clients for **GPT-4o** and **Claude 3 Sonnet** via **OpenRouter**, **Gemini 1.5 Flash** via **Google Generative AI**, and **DeepSeek** via its REST API.
- Reads API keys from environment (`OPENROUTER_API_KEY`, `GOOGLE_API_KEY`, `DEEPSEEK_API_KEY`).
- Exposes a `get_all_models()` factory returning a dictionary of ready-to-use model handles.

### `src/prompts.py` â€” Prompt sources & Mongo
- Loads prompts from CSV using `CSV_PROMPTS_PATH` (defaults to `/app/data/EA_Benchmark_Prompts_200.csv`).
- Connects to MongoDB database **`etl_processed`** using an Airflow Variable `mongo_uri`
  (default: `mongodb://root:example@mongo:27017/admin`).
- Supports loading prompts directly from Mongo collections when needed.

### `src/load.py` â€” Execution & persistence
- Iterates **all prompts Ã— all models**, handles per-model invocation nuances (e.g., DeepSeekâ€™s chat API).
- Persists documents into Mongo with fields:
  `prompt`, `response`, `model`, `timestamp (UTC ISO8601)`.
- Uses per-model collections, e.g.:
  `gpt4o_responses`, `claude_responses`, `gemini_responses`, `deepseek_responses`.

### `src/notebook.ipynb` â€” Analysis & reports
- Notebook for computing metrics, aggregating CSV results, and plotting comparisons.

---

## ðŸ§ª Metrics & Analysis
The benchmark focuses on contamination patterns in token sequences:

| Metric | What it measures | Why it matters |
|---|---|---|
| **Repetition Rate (RR)** | Lexical redundancy / n-gram loops | Flags degenerate loops and low-diversity outputs |
| **Entropy (H)** | Token distribution diversity | Low entropy â†”ï¸Ž repetitive or templated outputs |
| **Semantic Overlap (SO)** | Embedding similarity to prompts or reference corpora | Detects overfitting, prompt echoing, or leakage |
| **Contamination Index (CI)** | Composite of RR, H, SO (weighted) | Single score to rank agents by â€œcleanlinessâ€ |

> The notebook computes these from consolidated CSVs and visualizes per-model distributions, confidence intervals, and pairwise deltas.

---

## ðŸ“‚ Data & Artifacts
**Input & intermediate data (in `data/`):**
- `EA_Benchmark_Prompts_200.csv` â€” Canonical prompt set (200 items).
- `evaluaciones_con_claude.csv` â€” Raw evaluation outputs from Claude.
- `evaluaciones_con_claude_fixed.csv` â€” Cleaned/normalized Claude evaluations (stored in `src/` for convenience in this iteration).
- `model_outputs_clean.csv` â€” Consolidated and cleaned outputs for metric computation.
- (Batches) `model_outputs_batch_{0,25,50,75}.csv` â€” Optional staged outputs by chunking prompts.

> Tip: keep large raw generations in Git LFS or external storage to avoid bloating the repo.

---

## ðŸ“¦ Repository Structure
```bash
bigdatas/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ EA_Benchmark_Prompts_200.csv
â”‚   â”œâ”€â”€ evaluaciones_con_claude.csv
â”‚   â””â”€â”€ model_outputs_clean.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ invoke.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ prompts.py
â”‚   â”œâ”€â”€ notebook.ipynb
â”‚   â”œâ”€â”€ evaluaciones_con_claude_fixed.csv
â”‚   â”œâ”€â”€ model_outputs.csv
â”‚   â”œâ”€â”€ model_outputs_batch_0.csv
â”‚   â”œâ”€â”€ model_outputs_batch_25.csv
â”‚   â”œâ”€â”€ model_outputs_batch_50.csv
â”‚   â””â”€â”€ model_outputs_batch_75.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
